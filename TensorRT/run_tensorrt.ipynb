{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Running TensorRT by converting Frozen  Inference Model from TensorFlow\n",
    "\n",
    "This notebook has purpose to present how to use TensorRT from frozen model. Mobilenet_V2_1.0_224 model was used in the conversion due to its small model size and use less of computation resouce, also with Mobilenet model can get results accuracy and does not take long of running time.\n",
    "\n",
    "## Prerequisite\n",
    "1. Install Deepstream and TensorRT as describe in the top level of this repository.\n",
    "2. `$ pip install Pillow pycuda numpy`\n",
    "3. docker running on your local machine\n",
    "\n",
    "## Docker Guideline\n",
    "After finished TensorRT installation, try to run docker with the following command\n",
    "\n",
    "`$ docker run --gpus all -it -v path_to_local_host:path_to_docker -p 8888:8888 nvcr.io/nvidia/tlt-streamanalytics:v2.0_py3 /bin/bash`\n",
    "\n",
    "\n",
    "If docker cannot run with above command after you have restarted or shutdown computer, you can just simply type the following two commands.\n",
    "\n",
    "`$ sudo systemctl daemon-reload`\n",
    "\n",
    "`$ sudo systemctl restart docker`\n",
    "\n",
    "\n",
    "Then, you can try to run docker again.\n",
    "\n",
    "After docker launch, use the following command to convert the TensorFlow frozen inference model into TensorRT format which operates by TRT UFF parser.\n",
    "\n",
    "`$convert-to-uff mobilenet_v1_1.0_224_frozen.pb -o mobilenet_v1_1.0_224.uff`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the neccessary libraries, especially trt which allow to convert the desired TensorFlow frozen graph into TensorRT compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch size, this notebook is just for TensorRT testing, so it is not necessary to use batch size much. The precision mode is float 32 (FP32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BATCH_SIZE = 1\n",
    "MAX_WORKSPACE_SIZE = 1 << 30\n",
    "\n",
    "LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "DTYPE = trt.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commond model configuration including define model name, input and output properties, label file, loop times, and top number of classification result shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = 'mobilenet_v1_1.0_224.uff'\n",
    "INPUT_NAME = 'input'\n",
    "INPUT_SHAPE = (3, 224, 224)\n",
    "OUTPUT_NAME = 'MobilenetV1/Predictions/Reshape_1'\n",
    "\n",
    "LABELS = 'class_labels.txt'\n",
    "\n",
    "LOOP_TIMES = 10\n",
    "TOP_N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-allocate buffers (allowcate device memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_buffers(engine):\n",
    "    print('allocate buffers')\n",
    "    \n",
    "    h_input = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(DTYPE))\n",
    "    h_output = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(DTYPE))\n",
    "    d_input = cuda.mem_alloc(h_input.nbytes)\n",
    "    d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "    \n",
    "    return h_input, d_input, h_output, d_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting build network by register input and output properties into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_engine(model_file):\n",
    "    print('build engine...')\n",
    "\n",
    "    with trt.Builder(LOGGER) as builder, builder.create_network() as network, trt.UffParser() as parser:\n",
    "        builder.max_workspace_size = MAX_WORKSPACE_SIZE\n",
    "        builder.max_batch_size = MAX_BATCH_SIZE\n",
    "        if DTYPE == trt.float16:\n",
    "            builder.fp16_mode = True\n",
    "        parser.register_input(INPUT_NAME, INPUT_SHAPE, trt.UffInputOrder.NCHW)\n",
    "        parser.register_output(OUTPUT_NAME)\n",
    "        parser.parse(model_file, network, DTYPE)\n",
    "        \n",
    "        return builder.build_cuda_engine(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load input image and convert into image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input(img_path, host_buffer):\n",
    "    print('load input')\n",
    "    \n",
    "    with Image.open(img_path) as img:\n",
    "        c, h, w = INPUT_SHAPE\n",
    "        dtype = trt.nptype(DTYPE)\n",
    "        img_array = np.asarray(img.resize((w, h), Image.BILINEAR)).transpose([2, 0, 1]).astype(dtype).ravel()\n",
    "        # preprocess for mobilenet\n",
    "        img_array = img_array / 127.5 - 1.0\n",
    "        \n",
    "    np.copyto(host_buffer, img_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start inference with test image and print the inference time to terminal (Perform device to host memory copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference(n, context, h_input, d_input, h_output, d_output):\n",
    "    cuda.memcpy_htod(d_input, h_input)\n",
    "    \n",
    "    st = time.time()\n",
    "    context.execute(batch_size=1, bindings=[int(d_input), int(d_output)])\n",
    "    print('Inference time {}: {} [msec]'.format(n, (time.time() - st)*1000))\n",
    "\n",
    "    cuda.memcpy_dtoh(h_output, d_output)\n",
    "    \n",
    "    return h_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to run the testing dataset, starting by fetch input image into above functions and print the top 5 classes of classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build engine...\n",
      "allocate buffers\n",
      "load input\n",
      "Inference time 0: 0.6520748138427734 [msec]\n",
      "Inference time 1: 0.579833984375 [msec]\n",
      "Inference time 2: 0.5779266357421875 [msec]\n",
      "Inference time 3: 0.5776882171630859 [msec]\n",
      "Inference time 4: 0.5779266357421875 [msec]\n",
      "Inference time 5: 0.5795955657958984 [msec]\n",
      "Inference time 6: 0.5795955657958984 [msec]\n",
      "Inference time 7: 0.5772113800048828 [msec]\n",
      "Inference time 8: 0.5784034729003906 [msec]\n",
      "Inference time 9: 0.5786418914794922 [msec]\n",
      "\n",
      "Classification Result:\n",
      "1 water buffalo 0.5712370276451111\n",
      "2 oxygen mask 0.22553090751171112\n",
      "3 bison 0.07660061866044998\n",
      "4 wild boar 0.044601891189813614\n",
      "5 thunder snake 0.018330469727516174\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = 'testing_imgs\\testimg-07.jpg'\n",
    "\n",
    "    with open(LABELS) as f:\n",
    "        labels = f.read().split('\\n')\n",
    "        \n",
    "    with build_engine(MODEL_FILE) as engine:\n",
    "        h_input, d_input, h_output, d_output = allocate_buffers(engine)\n",
    "        load_input(args, h_input)\n",
    "        \n",
    "        with engine.create_execution_context() as context:\n",
    "            for i in range(LOOP_TIMES):\n",
    "                output = do_inference(i, context, h_input, d_input, h_output, d_output)\n",
    "\n",
    "    pred_idx = np.argsort(output)[::-1]\n",
    "    pred_prob = np.sort(output)[::-1]\n",
    "\n",
    "    print('\\nClassification Result:')\n",
    "    for i in range(TOP_N):\n",
    "        print('{} {} {}'.format(i + 1, labels[pred_idx[i]], pred_prob[i]))\n",
    "\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of testing image\n",
    "This image was taken by me at Nan province. Thai buffalo images might never have trained in pre-trained dataset, but the model prediction still got a acceptable result.\n",
    "\n",
    "\n",
    "Classification Result:\n",
    "1. water buffalo 0.5712370276451111\n",
    "2. oxygen mask 0.22553090751171112\n",
    "3. bison 0.07660061866044998\n",
    "4. wild boar 0.044601891189813614\n",
    "5. thunder snake 0.018330469727516174\n",
    "\n",
    "<img src=\"testing_imgs/testimg-07.jpg\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
